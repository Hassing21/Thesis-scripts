{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e3322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.activations import gelu\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# === SEQUENCE CREATION FUNCTION ===\n",
    "def create_sequences(data, features, target, sequence_length):\n",
    "    \"\"\"\n",
    "    Convert time series data into sequences for LSTM training\n",
    "    Returns:\n",
    "        X: 3D array of sequences (samples, time steps, features)\n",
    "        y: 1D array of target values\n",
    "        dates: Corresponding dates for each sequence\n",
    "    \"\"\"\n",
    "    X, y, dates = [], [], []\n",
    "    data_values = data[features].values\n",
    "    target_values = data[target].values\n",
    "    date_values = data.index.values\n",
    "    \n",
    "    for i in range(sequence_length, len(data)):\n",
    "        X.append(data_values[i-sequence_length:i])\n",
    "        y.append(target_values[i])\n",
    "        dates.append(date_values[i])\n",
    "    \n",
    "    return np.array(X), np.array(y), np.array(dates)\n",
    "\n",
    "# === SETTINGS ===\n",
    "TARGET = \"future_return_1d\"\n",
    "SEQUENCE_LENGTH = 30\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.003\n",
    "PURGE_DAYS = 5\n",
    "DROPOUT_RATE = 0.2\n",
    "L2_REG = 0.001\n",
    "\n",
    "top_features = [\n",
    "    'open',\n",
    "    'high',\n",
    "    'low',\n",
    "    'close',\n",
    "    'volume',\n",
    "    'rsi',\n",
    "    'ema_short',\n",
    "    'ema_long',\n",
    "    'volatility_atr',\n",
    "    'bb_width',\n",
    "    'obv',\n",
    "    'volume_norm',\n",
    "    'macd',\n",
    "    'macd_signal',\n",
    "    'macd_hist',\n",
    "    'return_1d',\n",
    "    'return_3d',\n",
    "    'return_7d',\n",
    "    'adx',\n",
    "    'hma_14',\n",
    "    'vwap',\n",
    "    'cmf',\n",
    "    'sentiment_news_z',\n",
    "    'sentiment_twitter_z'\n",
    "]\n",
    "\n",
    "# === DATA LOADING ===\n",
    "df = pd.read_csv(\"/kaggle/input/final-features/BTCUSDT_features_final_sentiment.csv\", parse_dates=[\"timestamp\"])\n",
    "df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "# === DROP NaNs ===\n",
    "df.dropna(subset=top_features + [TARGET], inplace=True)\n",
    "\n",
    "# === BUILD LSTM MODEL ===\n",
    "def build_lstm(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(\n",
    "            128, return_sequences=True, input_shape=input_shape,\n",
    "            dropout=DROPOUT_RATE, recurrent_dropout=DROPOUT_RATE / 2,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(L2_REG)\n",
    "        ),\n",
    "        tf.keras.layers.LSTM(\n",
    "            64, return_sequences=False,\n",
    "            dropout=DROPOUT_RATE, recurrent_dropout=DROPOUT_RATE / 2,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(L2_REG)\n",
    "        ),\n",
    "        tf.keras.layers.Dense(\n",
    "            32, activation='relu',\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(L2_REG)\n",
    "        ),\n",
    "        tf.keras.layers.Dropout(DROPOUT_RATE),\n",
    "        tf.keras.layers.Dense(\n",
    "            16, activation='relu',\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(L2_REG)\n",
    "        ),\n",
    "        tf.keras.layers.Dropout(DROPOUT_RATE),\n",
    "        tf.keras.layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# === FEATURE IMPORTANCE ===\n",
    "def calculate_rf_importance(X, y, features):\n",
    "    X_flat = X.reshape(X.shape[0], -1)\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_flat, y)\n",
    "    importances = rf.feature_importances_\n",
    "    grouped = {}\n",
    "    for i, feat in enumerate(features):\n",
    "        grouped[feat] = np.sum(importances[i::len(features)])\n",
    "    return grouped\n",
    "\n",
    "# === METRICS ===\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    corr, _ = pearsonr(y_true, y_pred)\n",
    "    hit_rate = np.mean(np.sign(y_true) == np.sign(y_pred))\n",
    "    return {\"r2\": r2, \"mae\": mae, \"rmse\": rmse, \"corr\": corr, \"hit_rate\": hit_rate}\n",
    "\n",
    "# === WINDOWS ===\n",
    "WINDOWS = [\n",
    "    {\"name\": \"W1\", \"train_end\": \"2021-09-30\", \"test_start\": \"2021-10-10\", \"test_end\": \"2021-12-31\"},\n",
    "    {\"name\": \"W2\", \"train_end\": \"2021-11-30\", \"test_start\": \"2021-12-10\", \"test_end\": \"2022-02-28\"},\n",
    "]\n",
    "\n",
    "results = []\n",
    "all_feature_importance = {}\n",
    "\n",
    "# === MAIN TRAINING LOOP ===\n",
    "for w in WINDOWS:\n",
    "    print(f\"\\n🚀 {w['name']} | Training until {w['train_end']} | Testing from {w['test_start']} to {w['test_end']}\")\n",
    "\n",
    "    train_end = pd.to_datetime(w[\"train_end\"])\n",
    "    test_start = pd.to_datetime(w[\"test_start\"])\n",
    "    test_end = pd.to_datetime(w[\"test_end\"])\n",
    "    purge_start = test_start - timedelta(days=PURGE_DAYS)\n",
    "\n",
    "    train = df[df.index <= purge_start]\n",
    "    test = df[(df.index >= test_start) & (df.index <= test_end)]\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = train.copy()\n",
    "    test_scaled = test.copy()\n",
    "    train_scaled[top_features] = scaler.fit_transform(train[top_features])\n",
    "    test_scaled[top_features] = scaler.transform(test[top_features])\n",
    "\n",
    "    # Sequences\n",
    "    X_train, y_train, _ = create_sequences(train_scaled, top_features, TARGET, SEQUENCE_LENGTH)\n",
    "    X_test, y_test, test_dates = create_sequences(test_scaled, top_features, TARGET, SEQUENCE_LENGTH)\n",
    "\n",
    "    if len(X_train) < 50 or len(X_test) < 10:\n",
    "        print(\" Not enough data, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Scale target\n",
    "    target_scaler = StandardScaler()\n",
    "    y_train_scaled = target_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_test_scaled = target_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = build_lstm((SEQUENCE_LENGTH, len(top_features)))\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ModelCheckpoint(f\"model_{w['name']}.h5\", save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train_scaled,\n",
    "        validation_split=0.1,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    preds_scaled = model.predict(X_test).flatten()\n",
    "    preds = target_scaler.inverse_transform(preds_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "    metrics = evaluate_metrics(y_test, preds)\n",
    "    metrics[\"ic\"], _ = spearmanr(y_test, preds)\n",
    "    hit_rate_scaled = np.mean(np.sign(y_test_scaled) == np.sign(preds_scaled))\n",
    "    print(f\"🔁 Hitrate (scaled): {hit_rate_scaled:.4f} | (inverse): {metrics['hit_rate']:.4f}\")\n",
    "    results.append({\"window\": w[\"name\"], **metrics})\n",
    "\n",
    "    importance = calculate_rf_importance(X_train, y_train_scaled, top_features)\n",
    "    all_feature_importance[w[\"name\"]] = importance\n",
    "\n",
    "    # Plot learning curve\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Val Loss\")\n",
    "    plt.title(f\"Learning Curve - {w['name']}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot residuals\n",
    "    residuals = y_test - preds\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(residuals, bins=20, edgecolor='black', color='skyblue', alpha=0.7)\n",
    "    plt.axvline(0, color='red', linestyle='--')\n",
    "    plt.title(f\"Residuals Distribution - {w['name']}\")\n",
    "    plt.xlabel(\"Residual\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print top features\n",
    "    print(f\"Top 3 features in {w['name']}:\")\n",
    "    for feat, imp in sorted(importance.items(), key=lambda x: x[1], reverse=True)[:3]:\n",
    "        print(f\"  {feat}: {imp:.4f}\")\n",
    "\n",
    "# === COMBINED RESULTS ===\n",
    "# Combined scatter plot\n",
    "all_preds, all_true, all_labels = [], [], []\n",
    "for i, w in enumerate(WINDOWS):\n",
    "    w_name = w[\"name\"]\n",
    "    test = df[w[\"test_start\"]:w[\"test_end\"]]\n",
    "    test_scaled = test.copy()\n",
    "    test_scaled[top_features] = scaler.transform(test[top_features])\n",
    "    X_test, y_test, _ = create_sequences(test_scaled, top_features, TARGET, SEQUENCE_LENGTH)\n",
    "    model = tf.keras.models.load_model(f\"model_{w_name}.h5\", compile=False)\n",
    "    preds = model.predict(X_test).flatten()\n",
    "    preds = target_scaler.inverse_transform(preds.reshape(-1, 1)).flatten()\n",
    "    all_preds.extend(preds)\n",
    "    all_true.extend(y_test)\n",
    "    all_labels.extend([w_name] * len(y_test))\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "sns.scatterplot(x=all_true, y=all_preds, hue=all_labels, alpha=0.6, palette=\"Set2\")\n",
    "min_val, max_val = min(min(all_true), min(all_preds)), max(max(all_true), max(all_preds))\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=1.5, label=\"Perfect Prediction\")\n",
    "plt.xlabel(\"Actual 1d Return\")\n",
    "plt.ylabel(\"Predicted 1d Return\")\n",
    "plt.title(\"Actual vs Predicted Returns (All Windows)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(title=\"Window\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Results dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n📊 Summary:\")\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Feature importance heatmap\n",
    "importance_df = pd.DataFrame(all_feature_importance).fillna(0)\n",
    "mean_importance = importance_df.mean(axis=1)\n",
    "sorted_features = mean_importance.sort_values(ascending=False).index\n",
    "importance_df_sorted = importance_df.loc[sorted_features]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(importance_df_sorted, annot=True, fmt=\".3f\", cmap=\"Blues\")\n",
    "plt.title(\"Feature Importance per Window (Ranked)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Average feature importance\n",
    "plt.figure(figsize=(8, 4))\n",
    "mean_importance.sort_values().plot(kind='barh', color='steelblue')\n",
    "plt.title(\"Average Feature Importance (All Windows)\")\n",
    "plt.xlabel(\"Mean Importance\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
