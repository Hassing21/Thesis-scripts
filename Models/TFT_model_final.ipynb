{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3576189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# === TFT COMPONENTS ===\n",
    "class GatedLinearUnit(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.linear = tf.keras.layers.Dense(units)\n",
    "        self.gate = tf.keras.layers.Dense(units, activation='sigmoid')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.linear(inputs) * self.gate(inputs)\n",
    "\n",
    "class VariableSelectionNetwork(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_features, hidden_units, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_features = num_features\n",
    "        self.hidden_units = hidden_units\n",
    "\n",
    "        # Layer components\n",
    "        self.flattened_grn = GatedResidualNetwork(hidden_units, dropout_rate)\n",
    "        self.single_variable_grns = [\n",
    "            GatedResidualNetwork(hidden_units, dropout_rate)\n",
    "            for _ in range(num_features)\n",
    "        ]\n",
    "        self.selection_dense = tf.keras.layers.Dense(num_features)\n",
    "        self.softmax = tf.keras.layers.Softmax(axis=-1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs shape: (batch, time, features)\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        time_steps = tf.shape(inputs)[1]\n",
    "\n",
    "        # Flatten to shape: (batch*time, features)\n",
    "        flattened = tf.reshape(inputs, (batch_size * time_steps, self.num_features))\n",
    "\n",
    "        # Get feature selection weights\n",
    "        selection_weights = self.flattened_grn(flattened)\n",
    "        selection_weights = self.selection_dense(selection_weights)\n",
    "        selection_weights = self.softmax(selection_weights)  # shape: (batch*time, num_features)\n",
    "\n",
    "        # Process each variable individually\n",
    "        processed_vars = []\n",
    "        for i in range(self.num_features):\n",
    "            var_input = flattened[:, i:i+1]  # shape: (batch*time, 1)\n",
    "            processed = self.single_variable_grns[i](var_input)  # shape: (batch*time, hidden_units)\n",
    "            processed_vars.append(processed)\n",
    "\n",
    "        # Concatenate and split per feature\n",
    "        processed_vars = tf.concat(processed_vars, axis=-1)  # shape: (batch*time, hidden_units * num_features)\n",
    "        split_vars = tf.split(processed_vars, self.num_features, axis=-1)\n",
    "\n",
    "        # Weight each variable output by its selection weight\n",
    "        weighted_vars = [\n",
    "            v * tf.expand_dims(selection_weights[:, i], axis=-1)\n",
    "            for i, v in enumerate(split_vars)\n",
    "        ]\n",
    "        selected = tf.add_n(weighted_vars)  # shape: (batch*time, hidden_units)\n",
    "\n",
    "        # Reshape back to (batch, time, hidden_units)\n",
    "        selected = tf.reshape(selected, (batch_size, time_steps, self.hidden_units))\n",
    "\n",
    "        return selected, selection_weights\n",
    "\n",
    "class GatedResidualNetwork(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_units, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.dense1 = tf.keras.layers.Dense(hidden_units, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(hidden_units)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.glu = GatedLinearUnit(hidden_units)\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        # Skip connection\n",
    "        skip = inputs\n",
    "        \n",
    "        # First dense layer\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        # Second dense layer\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        # GLU activation\n",
    "        x = self.glu(x)\n",
    "        \n",
    "        # Residual connection (if dimensions match)\n",
    "        if skip.shape[-1] == x.shape[-1]:\n",
    "            x = x + skip\n",
    "            \n",
    "        # Layer normalization\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=key_dim,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        attention_output = self.attention(inputs, inputs, training=training)\n",
    "        return self.layer_norm(inputs + attention_output)\n",
    "\n",
    "# === ENHANCED TFT MODEL WITH FEATURE IMPORTANCE EXTRACTION ===\n",
    "class TFTModel(tf.keras.Model):\n",
    "    def __init__(self, sequence_length, num_features, feature_names, hidden_units=128, num_heads=4, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_features = num_features\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "        # Store VSN for feature importance extraction\n",
    "        self.vsn = VariableSelectionNetwork(num_features, hidden_units, dropout_rate)\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm1 = tf.keras.layers.LSTM(\n",
    "            hidden_units, \n",
    "            return_sequences=True, \n",
    "            dropout=dropout_rate,\n",
    "            recurrent_dropout=dropout_rate/2\n",
    "        )\n",
    "        \n",
    "        self.lstm2 = tf.keras.layers.LSTM(\n",
    "            hidden_units//2, \n",
    "            return_sequences=True,\n",
    "            dropout=dropout_rate,\n",
    "            recurrent_dropout=dropout_rate/2\n",
    "        )\n",
    "        \n",
    "        # Attention\n",
    "        self.attention = MultiHeadAttention(num_heads, hidden_units//num_heads, dropout_rate)\n",
    "        \n",
    "        # Temporal processing\n",
    "        self.temporal_gln = GatedLinearUnit(hidden_units)\n",
    "        \n",
    "        # Output layers\n",
    "        self.dense1 = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dense2 = tf.keras.layers.Dense(16, activation='relu')\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.output_layer = tf.keras.layers.Dense(1, activation='linear')\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        # Variable Selection Network\n",
    "        selected_features, self.feature_weights = self.vsn(inputs)\n",
    "        \n",
    "        # LSTM processing\n",
    "        lstm1_out = self.lstm1(selected_features, training=training)\n",
    "        lstm2_out = self.lstm2(lstm1_out, training=training)\n",
    "        \n",
    "        # Attention\n",
    "        attention_out = self.attention(lstm2_out, training=training)\n",
    "        \n",
    "        # Temporal processing\n",
    "        temporal_features = self.temporal_gln(attention_out)\n",
    "        \n",
    "        # Take last time step\n",
    "        last_step = temporal_features[:, -1, :]\n",
    "        \n",
    "        # Output layers\n",
    "        x = self.dense1(last_step)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        output = self.output_layer(x)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_feature_importance(self, X_sample):\n",
    "        \"\"\"Extract feature importance from Variable Selection Network\"\"\"\n",
    "        # Get feature weights for a sample batch\n",
    "        _ = self(X_sample, training=False)  # Forward pass to compute weights\n",
    "        \n",
    "        # Average weights across batch and time\n",
    "        importance = tf.reduce_mean(self.feature_weights, axis=0).numpy()\n",
    "        \n",
    "        # Create feature importance dictionary\n",
    "        feature_importance = {\n",
    "            name: float(imp) for name, imp in zip(self.feature_names, importance)\n",
    "        }\n",
    "        \n",
    "        return feature_importance\n",
    "\n",
    "def build_tft_model(sequence_length, num_features, feature_names, hidden_units=128, num_heads=4, dropout_rate=0.2):\n",
    "    \"\"\"Build TFT model with feature importance capability\"\"\"\n",
    "    model = TFTModel(\n",
    "        sequence_length=sequence_length,\n",
    "        num_features=num_features, \n",
    "        feature_names=feature_names,\n",
    "        hidden_units=hidden_units,\n",
    "        num_heads=num_heads,\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    \n",
    "    # Build the model with a dummy input\n",
    "    dummy_input = tf.zeros((1, sequence_length, num_features))\n",
    "    model(dummy_input)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# === VISUALIZATION FUNCTIONS ===\n",
    "def plot_learning_curves(history, window_name):\n",
    "    \"\"\"Plot training and validation loss curves\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss', alpha=0.8)\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss', alpha=0.8)\n",
    "    plt.title(f'{window_name} - Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # MAE plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Training MAE', alpha=0.8)\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE', alpha=0.8)\n",
    "    plt.title(f'{window_name} - Training MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_importance(importance_dict, window_name, top_n=15):\n",
    "    \"\"\"Plot feature importance as horizontal bar chart\"\"\"\n",
    "    # Sort features by importance\n",
    "    sorted_features = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Take top N features\n",
    "    top_features = sorted_features[:top_n]\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    features, importances = zip(*top_features)\n",
    "    \n",
    "    plt.barh(range(len(features)), importances, alpha=0.8)\n",
    "    plt.yticks(range(len(features)), features)\n",
    "    plt.xlabel('Feature Importance (VSN Weight)')\n",
    "    plt.title(f'{window_name} - Top {top_n} Feature Importance')\n",
    "    plt.gca().invert_yaxis()  # Most important at top\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(importances):\n",
    "        plt.text(v + 0.001, i, f'{v:.3f}', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_feature_importance_ranking(all_importance):\n",
    "    \"\"\"Print top features ranked by average importance across all windows\"\"\"\n",
    "    # Kombiner alle feature importance dicts til DataFrame\n",
    "    importance_df = pd.DataFrame(all_importance).T  # shape: (features, windows)\n",
    "\n",
    "    # Beregn gennemsnit og sorter\n",
    "    importance_df['mean_importance'] = importance_df.mean(axis=1)\n",
    "    sorted_df = importance_df.sort_values('mean_importance', ascending=False)\n",
    "\n",
    "    # Udskriv top features\n",
    "    print(\"\\nüî¢ Feature Importance Ranking (avg. across windows):\")\n",
    "    for i, (feat, row) in enumerate(sorted_df.iterrows(), start=1):\n",
    "        print(f\"{i:>2}. {feat:<25} ‚Üí {row['mean_importance']:.4f}\")\n",
    "\n",
    "\n",
    "# === SEQUENCE CREATION (SAME AS ORIGINAL) ===\n",
    "def create_sequences(data, features, target, sequence_length):\n",
    "    X, y, dates = [], [], []\n",
    "    data_values = data[features].values\n",
    "    target_values = data[target].values\n",
    "    date_values = data.index.values\n",
    "    \n",
    "    for i in range(sequence_length, len(data)):\n",
    "        X.append(data_values[i-sequence_length:i])\n",
    "        y.append(target_values[i])\n",
    "        dates.append(date_values[i])\n",
    "    \n",
    "    return np.array(X), np.array(y), np.array(dates)\n",
    "\n",
    "# === METRICS (SAME AS ORIGINAL) ===\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    corr, _ = pearsonr(y_true, y_pred)\n",
    "    hit_rate = np.mean(np.sign(y_true) == np.sign(y_pred))\n",
    "    return {\"r2\": r2, \"mae\": mae, \"rmse\": rmse, \"corr\": corr, \"hit_rate\": hit_rate}\n",
    "\n",
    "# === SETTINGS ===\n",
    "TARGET = \"future_return_7d\"\n",
    "SEQUENCE_LENGTH = 30\n",
    "EPOCHS = 150 \n",
    "BATCH_SIZE = 32           \n",
    "LEARNING_RATE = 0.0001\n",
    "PURGE_DAYS = 5\n",
    "DROPOUT_RATE = 0.1 \n",
    "HIDDEN_UNITS = 128        \n",
    "NUM_HEADS = 2              \n",
    "\n",
    "\n",
    "top_features = [\n",
    "    'open', 'high', 'low', 'close', 'volume', 'rsi', 'ema_short', 'ema_long',\n",
    "    'volatility_atr', 'bb_width', 'obv', 'volume_norm', 'macd', 'macd_signal',\n",
    "    'macd_hist', 'return_1d', 'return_3d', 'return_7d', 'adx', 'hma_14',\n",
    "    'vwap', 'cmf', 'sentiment_news_z', 'sentiment_twitter_z'\n",
    "]\n",
    "\n",
    "# === WINDOWS ===\n",
    "WINDOWS = [\n",
    "    {\"name\": \"W1\", \"train_end\": \"2021-09-30\", \"test_start\": \"2021-10-10\", \"test_end\": \"2021-12-31\"},\n",
    "    {\"name\": \"W2\", \"train_end\": \"2021-11-30\", \"test_start\": \"2021-12-10\", \"test_end\": \"2022-02-28\"},\n",
    "]\n",
    "\n",
    "# === METRICS (SAME AS ORIGINAL) ===\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    corr, _ = pearsonr(y_true, y_pred)\n",
    "    hit_rate = np.mean(np.sign(y_true) == np.sign(y_pred))\n",
    "    return {\"r2\": r2, \"mae\": mae, \"rmse\": rmse, \"corr\": corr, \"hit_rate\": hit_rate}\n",
    "\n",
    "# === FEATURE IMPORTANCE RANKING FUNCTION ===\n",
    "def get_feature_importance_ranking_df(all_importance):\n",
    "    \"\"\"\n",
    "    Return a DataFrame with average importance across all windows.\n",
    "    Rows = features, columns = windows + mean_importance.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(all_importance).T  # shape: (features, windows)\n",
    "    df['mean_importance'] = df.mean(axis=1)\n",
    "    return df.sort_values('mean_importance', ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === MAIN USAGE EXAMPLE ===\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Load your data\n",
    "    df = pd.read_csv(\"/kaggle/input/final-features/BTCUSDT_features_final_sentiment.csv\", parse_dates=[\"timestamp\"])\n",
    "    df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "    df.dropna(subset=top_features + [TARGET], inplace=True)\n",
    "    \n",
    "    results = []\n",
    "    all_feature_importance = {}\n",
    "    window_names = []\n",
    "    \n",
    "    for w in WINDOWS:\n",
    "        print(f\"\\nüöÄ Enhanced TFT Training - {w['name']}\")\n",
    "        \n",
    "        # Data splitting logic\n",
    "        train_end = pd.to_datetime(w[\"train_end\"])\n",
    "        test_start = pd.to_datetime(w[\"test_start\"])\n",
    "        test_end = pd.to_datetime(w[\"test_end\"])\n",
    "        purge_start = test_start - timedelta(days=PURGE_DAYS)\n",
    "        \n",
    "        train = df[df.index <= purge_start]\n",
    "        test = df[(df.index >= test_start) & (df.index <= test_end)]\n",
    "        \n",
    "        # Scaling\n",
    "        scaler = StandardScaler()\n",
    "        train_scaled = train.copy()\n",
    "        test_scaled = test.copy()\n",
    "        train_scaled[top_features] = scaler.fit_transform(train[top_features])\n",
    "        test_scaled[top_features] = scaler.transform(test[top_features])\n",
    "        \n",
    "        # Create sequences\n",
    "        X_train, y_train, _ = create_sequences(train_scaled, top_features, TARGET, SEQUENCE_LENGTH)\n",
    "        X_test, y_test, test_dates = create_sequences(test_scaled, top_features, TARGET, SEQUENCE_LENGTH)\n",
    "        \n",
    "        if len(X_train) < 50 or len(X_test) < 10:\n",
    "            print(\"‚ö†Ô∏è Not enough data, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Scale target\n",
    "        target_scaler = StandardScaler()\n",
    "        y_train_scaled = target_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "        y_test_scaled = target_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Build TFT model\n",
    "        tf.keras.backend.clear_session()\n",
    "        model = build_tft_model(\n",
    "            sequence_length=SEQUENCE_LENGTH,\n",
    "            num_features=len(top_features),\n",
    "            feature_names=top_features,\n",
    "            hidden_units=HIDDEN_UNITS,\n",
    "            num_heads=NUM_HEADS,\n",
    "            dropout_rate=DROPOUT_RATE\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Compile\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(patience=8, factor=0.5),\n",
    "            tf.keras.callbacks.ModelCheckpoint(f\"tft_model_{w['name']}.h5\", save_best_only=True)\n",
    "        ]\n",
    "        \n",
    "        # Training\n",
    "        print(\"Training model...\")\n",
    "        history = model.fit(\n",
    "            X_train, y_train_scaled,\n",
    "            validation_split=0.1,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            verbose=1,\n",
    "            callbacks=callbacks,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Plot learning curves\n",
    "        plot_learning_curves(history, w['name'])\n",
    "        \n",
    "        # Predictions\n",
    "        preds_scaled = model.predict(X_test).flatten()\n",
    "        preds = target_scaler.inverse_transform(preds_scaled.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Evaluate\n",
    "        metrics = evaluate_metrics(y_test, preds)\n",
    "        metrics[\"ic\"], _ = spearmanr(y_test, preds)\n",
    "        \n",
    "        # Extract feature importance\n",
    "        print(\"Extracting feature importance...\")\n",
    "        sample_batch = X_test[:min(100, len(X_test))]  # Use sample for efficiency\n",
    "        feature_importance = model.get_feature_importance(sample_batch)\n",
    "        \n",
    "        # Store results\n",
    "        all_feature_importance[w['name']] = feature_importance\n",
    "        window_names.append(w['name'])\n",
    "        \n",
    "        # Plot individual feature importance\n",
    "        plot_feature_importance(feature_importance, w['name'])\n",
    "        \n",
    "        print(f\"üìä Enhanced TFT Results - {w['name']}:\")\n",
    "        print(f\"   R¬≤: {metrics['r2']:.4f}\")\n",
    "        print(f\"   Hit Rate: {metrics['hit_rate']:.4f}\")\n",
    "        print(f\"   Correlation: {metrics['corr']:.4f}\")\n",
    "        print(f\"   IC (Spearman): {metrics['ic']:.4f}\")\n",
    "        \n",
    "        results.append({\"window\": w[\"name\"], \"model\": \"Enhanced TFT\", **metrics})\n",
    "    \n",
    "   # === Feature Importance Summary Across Windows ===\n",
    "    if len(all_feature_importance) > 0:\n",
    "        print(\"\\n Overall Feature Importance (Average Across All Windows):\")\n",
    "        print_feature_importance_ranking(all_feature_importance)\n",
    "    \n",
    "       # === Overall Feature Importance Summary Across Windows ===\n",
    "        ranking_df = get_feature_importance_ranking_df(all_feature_importance)\n",
    "        \n",
    "        print(\"\\n Overall Feature Importance (Average Across All Windows):\")\n",
    "        print(ranking_df.head(20))  # viser top 10 samlet\n",
    "        \n",
    "        print(\"\\n Top 5 Features Across All Windows:\")\n",
    "        for feat, row in ranking_df.head(10).iterrows():\n",
    "            print(f\"   {feat}: {row['mean_importance']:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Results summary\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\n Enhanced TFT Summary:\")\n",
    "    print(results_df.round(4))\n",
    "    \n",
    "    # Feature importance summary\n",
    "    print(\"\\nüîç Feature Importance Summary:\")\n",
    "    for window, importance in all_feature_importance.items():\n",
    "        print(f\"\\n{window} - Top 5 Features:\")\n",
    "        sorted_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "        for feat, imp in sorted_features[:5]:\n",
    "            print(f\"   {feat}: {imp:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
